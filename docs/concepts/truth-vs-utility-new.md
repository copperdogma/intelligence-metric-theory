# Truth vs Utility in I=P/R

In I=P/R, **prediction is valued for utility**, not for “truth” as an end in itself.

A model can be:
- representationally wrong
- yet useful/predictive enough in a limited envelope

Example: geocentric celestial models can predict visible motion in limited regimes despite incorrect ontology.

---

## Two kinds of “truth”

1) **Representational truth**
   - Does the model’s story about what exists match reality?
2) **Mapping truth (utility)**
   - Does the model reliably map observations/actions to outcomes within the envelope?

I=P/R privileges mapping truth.

---

## The predictive / causal / mechanistic ladder (keep this)

This ladder describes increasing depth of model “truth” and robustness.

### 1) Predictive (in-distribution outcomes)
- Works inside familiar conditions
- Can succeed with correlations
- Breaks under intervention or shift

### 2) Causal (intervention robustness)
- Tracks what changes when you act
- Survives many interventions
- Distinguishes levers from mere correlates

### 3) Mechanistic (generative process fidelity)
- Matches how the system actually produces outcomes
- Supports novel extrapolation and design
- Most expensive to build, but often most portable

**Important:** deeper is not always better.
Choose the rung that maximizes I within the envelope (benefit vs cost).

---

## Coarsening can reduce “truth” but increase intelligence

Coarse models often discard true details that are:
- low leverage for the decision horizon
- drowned in noise
- not measurable/actionable

This can increase I by:
- reducing R
- keeping only high-signal, actionable structure

---

## When truth depth matters most

Truth depth becomes more valuable when:
- distribution shift is frequent
- interventions are common and high-stakes
- you must design/repair the system (mechanistic benefits)
- adversaries exist (correlation traps are exploited)

---

## Cargo cults as a failure of causal grip

Cargo cults copy surface correlates (predictive-looking patterns),
but lack causal/mechanistic leverage.
Their “model” consumes R without increasing actionable P.

This is a strong example of:
- representational narrative ≠ useful mapping
- consciousness ≠ prediction
- correlation ≠ control